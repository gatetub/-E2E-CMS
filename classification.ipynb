{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11182580,"sourceType":"datasetVersion","datasetId":6980136}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport h5py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Reshape\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\ndef read_images_from_hdf5(hdf5_file_path):\n    try:\n        with h5py.File(hdf5_file_path, 'r') as hf:\n            X = hf['X'][:]\n            y = hf['y'][:]\n            return X, y\n    except Exception as e:\n        print(f\"Error reading HDF5 file: {e}\")\n        return None, None\n\ndf1= '/kaggle/input/classify/SingleElectronPt50_IMGCROPS_n249k_RHv1 (1).hdf5'\nX1, y1 = read_images_from_hdf5(df1)\nX1 = X1.astype('float32') / 255.\ny1 = np.zeros(y1.shape[0])\n\n\ndf2 = '/kaggle/input/classify/SinglePhotonPt50_IMGCROPS_n249k_RHv1 (1) (1).hdf5'\nX2, y2 = read_images_from_hdf5(df2)\nX2 = X2.astype('float32') / 255.\ny2 = np.ones(y2.shape[0])\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:48:38.605566Z","iopub.execute_input":"2025-03-27T11:48:38.605905Z","iopub.status.idle":"2025-03-27T11:49:03.447584Z","shell.execute_reply.started":"2025-03-27T11:48:38.605881Z","shell.execute_reply":"2025-03-27T11:49:03.446747Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# prompt: create df3 contain X and y\n\nimport numpy as np\nX = np.concatenate((X1, X2), axis=0)\ny = np.concatenate((y1, y2), axis=0)\n\ndataset = (X, y)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:49:03.448838Z","iopub.execute_input":"2025-03-27T11:49:03.449564Z","iopub.status.idle":"2025-03-27T11:49:04.825834Z","shell.execute_reply.started":"2025-03-27T11:49:03.449531Z","shell.execute_reply":"2025-03-27T11:49:04.825136Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nimport cv2  # OpenCV for greyscaling\n\n# Assuming X and y are NumPy arrays\n# X = ... (your image data)\n# y = ... (your labels)\n\nif len(X.shape) == 4 and X.shape[-1] == 3:  # Check if it's a 4D RGB image\n    # Greyscale each image in the batch\n    X_greyscale = []\n    for img in X:\n        img_grey = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) #or cv2.COLOR_BGR2GRAY if your images are in BGR order\n        X_greyscale.append(img_grey)\n    X = np.array(X_greyscale)\n    X = np.expand_dims(X, axis=-1) # add a channel dimension to make it 4D again. (batch, height, width, 1)\n\nif len(X.shape) == 4:\n    X_reshaped = X.reshape(X.shape[0], -1)  # Flatten each image to a single vector\nelse:\n    X_reshaped = X\n\n# Perform Min-Max scaling for each pixel value across all images\nmin_val = np.min(X_reshaped)\nmax_val = np.max(X_reshaped)\n\nif max_val == min_val:\n    # Handle the case where all pixel values are the same to avoid division by zero\n    X = np.zeros_like(X_reshaped, dtype=float)\nelse:\n    X = (X_reshaped - min_val) / (max_val - min_val)\n\n# Reshape back to the original image format if necessary\nif len(X.shape) == 4:\n    X = X.reshape(X.shape)\n\n# Create the normalized dataset\ndataset = (X, y)\n\n# Now 'dataset' contains the greyscaled (if applicable) and Min-Max scaled data.\n# The scaled X is now stored in the variable 'X'.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T10:11:17.730964Z","iopub.execute_input":"2025-03-27T10:11:17.731264Z","iopub.status.idle":"2025-03-27T10:11:21.204119Z","shell.execute_reply.started":"2025-03-27T10:11:17.731244Z","shell.execute_reply":"2025-03-27T10:11:21.203430Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"if len(X.shape) == 4:\n    X_reshaped = X.reshape(X.shape[0], -1)  # Flatten each image to a single vector\nelse:\n    X_reshaped = X\n\n# Perform Min-Max scaling for each pixel value across all images\nmin_val = np.min(X_reshaped)\nmax_val = np.max(X_reshaped)\n\nif max_val == min_val:\n    # Handle the case where all pixel values are the same to avoid division by zero\n    X = np.zeros_like(X_reshaped, dtype=float)\nelse:\n    X = (X_reshaped - min_val) / (max_val - min_val)\n\n# Reshape back to the original image format if necessary\nif len(X.shape) == 4:\n    X = X.reshape(X.shape)\n\n# Create the normalized dataset\ndataset = (X, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T09:50:14.425752Z","iopub.execute_input":"2025-03-27T09:50:14.426052Z","iopub.status.idle":"2025-03-27T09:50:17.621515Z","shell.execute_reply.started":"2025-03-27T09:50:14.426031Z","shell.execute_reply":"2025-03-27T09:50:17.620793Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#exp\n#try\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport numpy as np\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion * planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\nclass ResNet15Approx(nn.Module):\n    def __init__(self, num_classes=1, input_channels=2):\n        super(ResNet15Approx, self).__init__()\n        self.in_planes = 32 #reduced from 64\n        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1, bias=False) #reduced from 64\n        self.bn1 = nn.BatchNorm2d(32)\n        self.layer1 = self._make_layer(BasicBlock, 32, 1, stride=1) #reduced from 64\n        self.layer2 = self._make_layer(BasicBlock, 64, 1, stride=2) #reduced from 128\n        self.layer3 = self._make_layer(BasicBlock, 64, 1, stride=2) #reduced from 256\n        self.layer4 = self._make_layer(BasicBlock, 64, 1, stride=2) #reduced from 512\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.linear = nn.Linear(256 * BasicBlock.expansion, num_classes) #reduced from 512\n        self.sigmoid = nn.Sigmoid()  # For binary classification\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avgpool(out)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        out = self.sigmoid(out) # Apply sigmoid for binary output\n        return out\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T10:19:25.476476Z","iopub.execute_input":"2025-03-27T10:19:25.476799Z","iopub.status.idle":"2025-03-27T10:19:25.493531Z","shell.execute_reply.started":"2025-03-27T10:19:25.476776Z","shell.execute_reply":"2025-03-27T10:19:25.492710Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"#try\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport numpy as np\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion * planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\nclass ResNet15Approx(nn.Module):\n    def __init__(self, num_classes=1, input_channels=2):\n        super(ResNet15Approx, self).__init__()\n        self.in_planes = 32 #reduced from 64\n        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1, bias=False) #reduced from 64\n        self.bn1 = nn.BatchNorm2d(32)\n        self.layer1 = self._make_layer(BasicBlock, 32, 1, stride=1) #reduced from 64\n        self.layer2 = self._make_layer(BasicBlock, 64, 1, stride=2) #reduced from 128\n        self.layer3 = self._make_layer(BasicBlock, 128, 1, stride=2) #reduced from 256\n        self.layer4 = self._make_layer(BasicBlock, 256, 1, stride=2) #reduced from 512\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.linear = nn.Linear(256 * BasicBlock.expansion, num_classes) #reduced from 512\n        self.sigmoid = nn.Sigmoid()  # For binary classification\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avgpool(out)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        out = self.sigmoid(out) # Apply sigmoid for binary output\n        return out\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T10:20:12.543726Z","iopub.execute_input":"2025-03-27T10:20:12.544094Z","iopub.status.idle":"2025-03-27T10:20:12.555575Z","shell.execute_reply.started":"2025-03-27T10:20:12.544067Z","shell.execute_reply":"2025-03-27T10:20:12.554611Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport numpy as np\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion * planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\nclass ResNet15Approx(nn.Module):\n    def __init__(self, num_classes=1, input_channels=2): #binary classification, 2 channels\n        super(ResNet15Approx, self).__init__()\n        self.in_planes = 64\n        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1) # Changed num_blocks to 2\n        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2) # Changed num_blocks to 2\n        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2) # Changed num_blocks to 2\n        self.layer4 = self._make_layer(BasicBlock, 512, 1, stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.linear = nn.Linear(512 * BasicBlock.expansion, num_classes)\n        self.sigmoid = nn.Sigmoid()  # For binary classification\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avgpool(out)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        out = self.sigmoid(out) # Apply sigmoid for binary output\n        return out\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T07:52:07.419934Z","iopub.execute_input":"2025-03-27T07:52:07.420266Z","iopub.status.idle":"2025-03-27T07:52:07.432256Z","shell.execute_reply.started":"2025-03-27T07:52:07.420237Z","shell.execute_reply":"2025-03-27T07:52:07.431406Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32).permute(0, 3, 1, 2) # NCHW format\n        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1) # Ensure y is a column vector\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n# Example usage:\nX = np.random.rand(498000, 32, 32, 2)\ny = np.random.randint(0, 2, 498000)\n\ndataset = CustomDataset(X, y)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T07:52:33.322244Z","iopub.execute_input":"2025-03-27T07:52:33.322660Z","iopub.status.idle":"2025-03-27T07:52:43.937623Z","shell.execute_reply.started":"2025-03-27T07:52:33.322627Z","shell.execute_reply":"2025-03-27T07:52:43.936894Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T07:52:47.929821Z","iopub.execute_input":"2025-03-27T07:52:47.930151Z","iopub.status.idle":"2025-03-27T07:52:47.977923Z","shell.execute_reply.started":"2025-03-27T07:52:47.930122Z","shell.execute_reply":"2025-03-27T07:52:47.976970Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"net = ResNet15Approx()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T09:53:37.707216Z","iopub.execute_input":"2025-03-27T09:53:37.707502Z","iopub.status.idle":"2025-03-27T09:53:37.761272Z","shell.execute_reply.started":"2025-03-27T09:53:37.707481Z","shell.execute_reply":"2025-03-27T09:53:37.760592Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nimport numpy as np\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T09:53:41.439394Z","iopub.execute_input":"2025-03-27T09:53:41.439720Z","iopub.status.idle":"2025-03-27T09:53:41.443614Z","shell.execute_reply.started":"2025-03-27T09:53:41.439691Z","shell.execute_reply":"2025-03-27T09:53:41.442826Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"net = ResNet15Approx().cuda()  # Move the model to the GPU\ncriterion = nn.BCELoss().cuda() #Move loss to GPU.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T08:25:37.035000Z","iopub.execute_input":"2025-03-27T08:25:37.035304Z","iopub.status.idle":"2025-03-27T08:25:37.057226Z","shell.execute_reply.started":"2025-03-27T08:25:37.035281Z","shell.execute_reply":"2025-03-27T08:25:37.056394Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"optimizer = optim.Adam(net.parameters(), lr=0.001)\n\nnum_epochs = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T08:25:52.691162Z","iopub.execute_input":"2025-03-27T08:25:52.691508Z","iopub.status.idle":"2025-03-27T08:25:52.696902Z","shell.execute_reply.started":"2025-03-27T08:25:52.691478Z","shell.execute_reply":"2025-03-27T08:25:52.695958Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"optimizer = optim.Adam(net.parameters(), lr=0.001)\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    net.train()\n    running_loss = 0.0\n    start_time = time.time()\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.cuda(), labels.cuda()  # Move data to GPU\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    epoch_time = time.time() - start_time\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}, Time: {epoch_time:.2f}s')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T08:06:53.097784Z","iopub.execute_input":"2025-03-27T08:06:53.098095Z","iopub.status.idle":"2025-03-27T08:08:49.098609Z","shell.execute_reply.started":"2025-03-27T08:06:53.098070Z","shell.execute_reply":"2025-03-27T08:08:49.097398Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-841bac4d95e0>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    print(\"CUDA is available!\")\n    device = torch.device(\"cuda\")\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"CUDA is NOT available. Check your installation!\")\n    device = torch.device(\"cpu\") #If cuda is not available, use cpu.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T09:54:00.439540Z","iopub.execute_input":"2025-03-27T09:54:00.439801Z","iopub.status.idle":"2025-03-27T09:54:00.444718Z","shell.execute_reply.started":"2025-03-27T09:54:00.439781Z","shell.execute_reply":"2025-03-27T09:54:00.444050Z"}},"outputs":[{"name":"stdout","text":"CUDA is available!\nUsing GPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\nimport numpy as np\n\n# Assuming X and y are NumPy arrays\nX = np.random.rand(498000, 32, 32, 2)\ny = np.random.randint(0, 2, 498000)\n\n# Convert NumPy arrays to PyTorch tensors\nX_tensor = torch.tensor(X, dtype=torch.float32).permute(0, 3, 1, 2)\ny_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n\n# Create TensorDataset\ndataset = TensorDataset(X_tensor, y_tensor)\n\n# Split dataset\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T10:15:44.020487Z","iopub.execute_input":"2025-03-27T10:15:44.020785Z","iopub.status.idle":"2025-03-27T10:15:55.388243Z","shell.execute_reply.started":"2025-03-27T10:15:44.020764Z","shell.execute_reply":"2025-03-27T10:15:55.387457Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"\n\nnet = ResNet15Approx().cuda() # Move model to GPU\ncriterion = nn.BCELoss().cuda() # Move loss to GPU\noptimizer = optim.Adam(net.parameters(), lr=0.001)\n\nnum_epochs = 10\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:32:42.264879Z","iopub.execute_input":"2025-03-27T11:32:42.265467Z","iopub.status.idle":"2025-03-27T11:32:42.296086Z","shell.execute_reply.started":"2025-03-27T11:32:42.265425Z","shell.execute_reply":"2025-03-27T11:32:42.295347Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"\nfor epoch in range(num_epochs):\n    net.train()\n    running_loss = 0.0\n    start_time = time.time()\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.cuda(), labels.cuda() # Move data to GPU\n\n\n        #print(f\"Input device: {inputs.device}, Label device: {labels.device}\")\n \n        optimizer.zero_grad()\n        outputs = net(inputs)\n        #print(f\"Output device: {outputs.device}, Model weight device: {next(net.parameters()).device}\")\n\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    epoch_time = time.time() - start_time\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}, Time: {epoch_time:.2f}s')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T10:20:25.292551Z","iopub.execute_input":"2025-03-27T10:20:25.292835Z","iopub.status.idle":"2025-03-27T10:29:24.324760Z","shell.execute_reply.started":"2025-03-27T10:20:25.292816Z","shell.execute_reply":"2025-03-27T10:29:24.323822Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 0.6945, Time: 54.00s\nEpoch [2/10], Loss: 0.6936, Time: 53.77s\nEpoch [3/10], Loss: 0.6933, Time: 54.54s\nEpoch [4/10], Loss: 0.6930, Time: 53.59s\nEpoch [5/10], Loss: 0.6927, Time: 53.72s\nEpoch [6/10], Loss: 0.6915, Time: 54.08s\nEpoch [7/10], Loss: 0.6884, Time: 53.79s\nEpoch [8/10], Loss: 0.6815, Time: 53.69s\nEpoch [9/10], Loss: 0.6687, Time: 54.04s\nEpoch [10/10], Loss: 0.6491, Time: 53.80s\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"net.eval()  # Set the model to evaluation mode\nwith torch.no_grad():  # Disable gradient calculation for evaluation\n    correct = 0\n    total = 0\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.cuda(), labels.cuda()  # Move test data to GPU\n        outputs = net(inputs)\n        predicted = (outputs > 0.5).float()  # Threshold at 0.5 for binary classification\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    print(f'Test Accuracy: {accuracy:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T10:33:23.515661Z","iopub.execute_input":"2025-03-27T10:33:23.516017Z","iopub.status.idle":"2025-03-27T10:33:27.274870Z","shell.execute_reply.started":"2025-03-27T10:33:23.515985Z","shell.execute_reply":"2025-03-27T10:33:27.274024Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 49.81%\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"num_epochs = 4\nfor epoch in range(num_epochs):\n    net.train()\n    running_loss = 0.0\n    start_time = time.time()\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.cuda(), labels.cuda() # Move data to GPU\n\n\n        #print(f\"Input device: {inputs.device}, Label device: {labels.device}\")\n \n        optimizer.zero_grad()\n        outputs = net(inputs)\n        #print(f\"Output device: {outputs.device}, Model weight device: {next(net.parameters()).device}\")\n\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    epoch_time = time.time() - start_time\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}, Time: {epoch_time:.2f}s')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T10:29:30.165133Z","iopub.execute_input":"2025-03-27T10:29:30.165473Z","iopub.status.idle":"2025-03-27T10:33:05.163827Z","shell.execute_reply.started":"2025-03-27T10:29:30.165446Z","shell.execute_reply":"2025-03-27T10:33:05.162967Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/4], Loss: 0.6214, Time: 54.15s\nEpoch [2/4], Loss: 0.5858, Time: 53.65s\nEpoch [3/4], Loss: 0.5423, Time: 53.22s\nEpoch [4/4], Loss: 0.4922, Time: 53.97s\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import time\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\nnet.eval()  # Set the model to evaluation mode\nwith torch.no_grad():  # Disable gradient calculation for evaluation\n    all_labels = []\n    all_probs = []\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.cuda(), labels.cuda()  # Move test data to GPU\n        outputs = net(inputs)\n        all_labels.extend(labels.cpu().numpy())\n        all_probs.extend(outputs.cpu().numpy())\n\n    fpr, tpr, thresholds = roc_curve(np.array(all_labels), np.array(all_probs))\n    roc_auc = auc(fpr, tpr)\n\n    plt.figure()\n    plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC)')\n    plt.legend(loc=\"lower right\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T10:33:12.712709Z","iopub.execute_input":"2025-03-27T10:33:12.713046Z","iopub.status.idle":"2025-03-27T10:33:16.847124Z","shell.execute_reply.started":"2025-03-27T10:33:12.713024Z","shell.execute_reply":"2025-03-27T10:33:16.846012Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":29},{"cell_type":"markdown","source":"**#RES18**# \n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import models, transforms\nimport numpy as np\n\n# Define a custom dataset class\nclass PhotonElectronDataset(Dataset):\n    def __init__(self, X, y, transform=None):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1) # Reshape y for binary cross-entropy\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        sample = self.X[idx]\n        label = self.y[idx]\n        if self.transform:\n            # Apply transformations if needed, for image-like data\n            # Assuming your data is like (batch_size, 32, 32, 2)\n            # You might need to adjust the transform to handle the 2 channels\n            sample = self.transform(sample.permute(2, 0, 1)) # (2, 32, 32)\n            sample = sample.unsqueeze(0) # (1, 2, 32, 32) - ResNet expects (batch_size, channels, height, width)\n        return sample, label\n\n# Load your data\n# Assuming X and y are numpy arrays\n# X shape: (498000, 32, 32, 2)\n# y shape: (498000,)\n# Replace this with your actual data loading\n# For demonstration, let's create dummy data\nnum_samples = 498000\nimg_size = 32\nnum_channels = 2\nX_dummy = np.random.rand(num_samples, img_size, img_size, num_channels)\ny_dummy = np.random.randint(0, 2, num_samples) # Binary labels (0 or 1)\n\n# Define transformations (if needed for ResNet input)\n# ResNet models typically expect 3-channel images (RGB)\n# You might need to adapt the input to match this or modify the first layer\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224, 224)), # ResNet input size\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet normalization\n])\n\n# Create the dataset\ndataset = PhotonElectronDataset(X_dummy, y_dummy, transform=transform)\n\n# Split the dataset into training and validation sets\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n# Create DataLoaders\n\n\n# Load a pretrained ResNet-18 model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:30:44.922747Z","iopub.execute_input":"2025-03-27T11:30:44.923084Z","iopub.status.idle":"2025-03-27T11:30:55.893021Z","shell.execute_reply.started":"2025-03-27T11:30:44.923057Z","shell.execute_reply":"2025-03-27T11:30:55.892264Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"batch_size = 256\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:31:39.106145Z","iopub.execute_input":"2025-03-27T11:31:39.106464Z","iopub.status.idle":"2025-03-27T11:31:39.110541Z","shell.execute_reply.started":"2025-03-27T11:31:39.106440Z","shell.execute_reply":"2025-03-27T11:31:39.109597Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"model = models.resnet18(pretrained=True)\nnum_channels=2\n# Modify the first convolutional layer to accept 2 input channels\n# Get the number of input channels of the first layer\nin_channels = model.conv1.in_channels\nif in_channels != num_channels:\n    model.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\n# Modify the final fully connected layer for binary classification\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 1)\n\n# Define the loss function and optimizer\ncriterion = nn.BCEWithLogitsLoss() # Use BCEWithLogitsLoss for numerical stability\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Move the model to the device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:31:41.683319Z","iopub.execute_input":"2025-03-27T11:31:41.683654Z","iopub.status.idle":"2025-03-27T11:31:41.917857Z","shell.execute_reply.started":"2025-03-27T11:31:41.683626Z","shell.execute_reply":"2025-03-27T11:31:41.916966Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(2, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1, bias=True)\n)"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"# Training loop\nnum_epochs = 10 # You can adjust the number of epochs\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for i, (inputs, labels) in enumerate(train_loader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        if (i + 1) % 100 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}')\n            running_loss = 0.0\n\n    # Validation loop (optional but recommended)\n    model.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        val_loss = 0.0\n        for inputs, labels in test_loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            predicted = torch.sigmoid(outputs) > 0.5\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss/len(test_loader):.4f}, Validation Accuracy: {100 * correct / total:.2f}%')\n\nprint('Finished Training')\n\n# You can now save the trained model\n# torch.save(model.state_dict(), 'resnet18_photon_electron_classifier.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:32:47.625542Z","iopub.execute_input":"2025-03-27T11:32:47.625836Z","iopub.status.idle":"2025-03-27T11:32:47.699777Z","shell.execute_reply.started":"2025-03-27T11:32:47.625815Z","shell.execute_reply":"2025-03-27T11:32:47.698534Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-490ee8a98236>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-43-90539c05bf9d>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Assuming your data is like (batch_size, 32, 32, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# You might need to adjust the transform to handle the 2 channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (2, 32, 32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (1, 2, 32, 32) - ResNet expects (batch_size, channels, height, width)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0"],"ename":"RuntimeError","evalue":"The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0","output_type":"error"}],"execution_count":48},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import models, transforms\nimport numpy as np\n\n# Define a custom dataset class\nclass PhotonElectronDataset(Dataset):\n    def __init__(self, X, y, transform=None):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1) # Reshape y for binary cross-entropy\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        sample = self.X[idx]\n        label = self.y[idx]\n        if self.transform:\n            sample = sample.permute(2, 0, 1) # (2, 32, 32)\n            sample = self.transform(sample)\n            # print(f\"Shape after transform: {sample.shape}\") # You can keep this for debugging\n            # if len(sample.shape) == 3: # Removed this condition and the next line\n            #     sample = sample.unsqueeze(0)\n        return sample, label\n\n# Load your data\n# Assuming X and y are numpy arrays\n# X shape: (498000, 32, 32, 2)\n# y shape: (498000,)\n# Replace this with your actual data loading\n# For demonstration, let's create dummy data\nnum_samples = 498000\nimg_size = 32\nnum_channels = 2\nX_dummy = np.random.rand(num_samples, img_size, img_size, num_channels)\ny_dummy = np.random.randint(0, 2, num_samples) # Binary labels (0 or 1)\n\n# Define transformations (if needed for ResNet input)\n# ResNet models typically expect 3-channel images (RGB)\n# You might need to adapt the input to match this or modify the first layer\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224, 224)), # ResNet input size\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5], std=[0.5, 0.5]) # Adjusted for 2 channels\n])\n\n# Create the dataset\ndataset = PhotonElectronDataset(X_dummy, y_dummy, transform=transform)\n\n# Split the dataset into training and validation sets\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n# Create DataLoaders\nbatch_size = 128\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\n\n# Load a pretrained ResNet-18 model\nmodel = models.resnet18(pretrained=True)\n\n# Modify the first convolutional layer to accept 2 input channels\n# Get the number of input channels of the first layer\nin_channels = model.conv1.in_channels\nif in_channels != num_channels:\n    model.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\n# Modify the final fully connected layer for binary classification\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 1)\n\n# Define the loss function and optimizer\ncriterion = nn.BCEWithLogitsLoss() # Use BCEWithLogitsLoss for numerical stability\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# Move the model to the device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:16:57.158873Z","iopub.execute_input":"2025-03-27T12:16:57.159229Z","iopub.status.idle":"2025-03-27T12:17:08.053840Z","shell.execute_reply.started":"2025-03-27T12:16:57.159205Z","shell.execute_reply":"2025-03-27T12:17:08.052809Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(2, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1, bias=True)\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Training loop\nnum_epochs = 10 # You can adjust the number of epochs\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for i, (inputs, labels) in enumerate(train_loader):\n        \n    \n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n\n        try:\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n            if (i + 1) % 100 == 0:\n                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}')\n                running_loss = 0.0\n        except RuntimeError as e:\n            print(f\"RuntimeError during forward/backward pass at epoch {epoch+1}, step {i+1}: {e}\")\n            print(f\"Input shape: {inputs.shape}\")\n            print(f\"Expected input shape for ResNet: (batch_size, channels, height, width)\")\n            continue # Skip this batch and continue with the next\n\n    # Validation loop (optional but recommended)\n    model.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        val_loss = 0.0\n        for inputs, labels in val_loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            try:\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                predicted = torch.sigmoid(outputs) > 0.5\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n            except RuntimeError as e:\n                print(f\"RuntimeError during validation at epoch {epoch+1}: {e}\")\n                print(f\"Input shape during validation: {inputs.shape}\")\n                continue # Skip this batch\n\n        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {100 * correct / total:.2f}%')\n\nprint('Finished Training')\n\n# You can now save the trained model\n# torch.save(model.state_dict(), 'resnet18_photon_electron_classifier.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:18:20.799714Z","iopub.execute_input":"2025-03-27T12:18:20.800068Z","iopub.status.idle":"2025-03-27T12:43:27.886407Z","shell.execute_reply.started":"2025-03-27T12:18:20.800042Z","shell.execute_reply":"2025-03-27T12:43:27.884994Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/10], Step [100/3113], Loss: 0.7504\nEpoch [1/10], Step [200/3113], Loss: 0.7046\nEpoch [1/10], Step [300/3113], Loss: 0.7000\nEpoch [1/10], Step [400/3113], Loss: 0.6949\nEpoch [1/10], Step [500/3113], Loss: 0.6960\nEpoch [1/10], Step [600/3113], Loss: 0.6950\nEpoch [1/10], Step [700/3113], Loss: 0.6939\nEpoch [1/10], Step [800/3113], Loss: 0.6938\nEpoch [1/10], Step [900/3113], Loss: 0.6936\nEpoch [1/10], Step [1000/3113], Loss: 0.6937\nEpoch [1/10], Step [1100/3113], Loss: 0.6933\nEpoch [1/10], Step [1200/3113], Loss: 0.6933\nEpoch [1/10], Step [1300/3113], Loss: 0.6932\nEpoch [1/10], Step [1400/3113], Loss: 0.6932\nEpoch [1/10], Step [1500/3113], Loss: 0.6933\nEpoch [1/10], Step [1600/3113], Loss: 0.6932\nEpoch [1/10], Step [1700/3113], Loss: 0.6934\nEpoch [1/10], Step [1800/3113], Loss: 0.6933\nEpoch [1/10], Step [1900/3113], Loss: 0.6933\nEpoch [1/10], Step [2000/3113], Loss: 0.6933\nEpoch [1/10], Step [2100/3113], Loss: 0.6933\nEpoch [1/10], Step [2200/3113], Loss: 0.6930\nEpoch [1/10], Step [2300/3113], Loss: 0.6934\nEpoch [1/10], Step [2400/3113], Loss: 0.6934\nEpoch [1/10], Step [2500/3113], Loss: 0.6933\nEpoch [1/10], Step [2600/3113], Loss: 0.6932\nEpoch [1/10], Step [2700/3113], Loss: 0.6933\nEpoch [1/10], Step [2800/3113], Loss: 0.6935\nEpoch [1/10], Step [2900/3113], Loss: 0.6933\nEpoch [1/10], Step [3000/3113], Loss: 0.6933\nEpoch [1/10], Step [3100/3113], Loss: 0.6933\nEpoch [1/10], Validation Loss: 0.6936, Validation Accuracy: 49.79%\nEpoch [2/10], Step [100/3113], Loss: 0.6934\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-ff0b71118f8e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-3b3f52033086>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (2, 32, 32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;31m# print(f\"Shape after transform: {sample.shape}\") # You can keep this for debugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# if len(sample.shape) == 3: # Removed this condition and the next line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;31m# handle PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mmode_to_nptype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"I\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"I;16\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyteorder\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"little\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"I;16B\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"F\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_to_nptype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36m__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"L\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m             \u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"typestr\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_conv_type_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mtobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0;31m# unpack data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_getencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0mbufsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m65536\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# see RawEncode.c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mim\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImagingCore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_im\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeferredError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_im\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_im\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}